{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1558ab",
   "metadata": {},
   "source": [
    "# Lösungsansätze für Regressionsprobleme bei hochdimensionalen Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69d6ae",
   "metadata": {},
   "source": [
    "## Kurzhinweise\n",
    "Sie können eine Zelle ausführen, indem Sie in die Zelle klicken und dann \"Shift + Enter\" drücken.\n",
    "\n",
    "Bitte führen Sie die Zellen nacheinander aus, da einige Zellen auf den Input vorheriger Zellen angewiesen sind. Falls Sie \"mitten im Notebook\" anfangen werden Sie das aber ggf. merken, wenn Sie eine Fehlermeldung erhalten.\n",
    "\n",
    "Die nächste Zelle bitte ausführen aber **nicht** verändern. Sie sorgt für das Layout des Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e0dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def _set_css_style(css_file_path):\n",
    "   \"\"\"\n",
    "   Read the custom CSS file and load it into Jupyter.\n",
    "   Pass the file path to the CSS file.\n",
    "   \"\"\"\n",
    "\n",
    "   styles = open(css_file_path, \"r\").read()\n",
    "   s = '<style>%s</style>' % styles     \n",
    "   return HTML(s)\n",
    "\n",
    "_set_css_style('assets/styles.css')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eae630",
   "metadata": {},
   "source": [
    "## Import der benötigten Pakete\n",
    "Die nächste Zelle müssen Sie ausführen, um die benötigten Python Pakete für das Notebook zu importieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68241d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pakete für Machine Learning\n",
    "from sklearn.datasets import make_regression # zur Generierung der Daten\n",
    "from sklearn.model_selection import train_test_split #zur Aufteilung des Datensatzes in Trainings- und Testdaten\n",
    "# Modelle\n",
    "from sklearn.linear_model import LinearRegression # zur Durchführung der linearen Regression\n",
    "from sklearn.linear_model import Ridge # zur Durchführung der Ridge Regression\n",
    "from sklearn.linear_model import Lasso # zur Durchführung der Ridge Regression\n",
    "from sklearn.linear_model import ElasticNet # zur Durchführung der Ridge Regression\n",
    "\n",
    "# Pakete zur Visualisierung und Ausgabe der Ergebnisse\n",
    "import matplotlib.pyplot as plt #zur Visualisierung\n",
    "from matplotlib.pyplot import cm #Für Farbcodierung der Geraden\n",
    "from prettytable import PrettyTable # zur Ausgabe der Ergebnisse\n",
    "\n",
    "# Pakete zur Berechnung\n",
    "import numpy as np #zur Nutzung einiger nützlicher Funktionen\n",
    "import random # zur Erzeugung von Zufallszahlen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af110cfa",
   "metadata": {},
   "source": [
    "## Generierung der Daten\n",
    "Die nächste Zelle müssen Sie ausführen, um die Daten für die Beispiele zu generieren. Probieren Sie gerne andere Beispiele aus und generieren Sie andere Daten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e324152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generiert ein zufälliges Regressionsproblem\n",
    "X, y = make_regression(n_samples = 30, n_features = 1, random_state=2, noise=30)\n",
    "\n",
    "# Skaliert abhängige Variable x (Berufserfahrung in Jahren) zwischen 0 - 20\n",
    "X = np.interp(X, (X.min(), X.max()), (0, 20))\n",
    "\n",
    "# Skaliert abhängige Variable y (Gehalt) zwischen 20,000..150,000 \n",
    "y = np.interp(y, (y.min(), y.max()), (20, 150))\n",
    "\n",
    "# Festlegung des Seeds, um Ergebnisse reproduzieren zu können. Somit werden immer die selben Zufallszahlen gezogen \n",
    "# Im Folgenden werden die 2 Beobachtungen für die Beispiele in der Vorlesung festgelegt.\n",
    "random.seed(30) \n",
    "rand1 = random.randint(1, len(X)) # Ziehen der 1. ganzzahligen Zufallszahl\n",
    "rand2 = random.randint(1, len(X)) # Ziehen der 2. ganzzahligen Zufallszahl\n",
    "random.seed(25) \n",
    "rand3 = random.randint(1, len(X)) # Ziehen der 3. ganzzahligen Zufallszahl\n",
    " \n",
    "point=[[rand1, rand2], [rand2, rand3]] #Das sind die Indizes der 4 Punkte, die für die beiden Beispiele gezogen werden \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c395e4f",
   "metadata": {},
   "source": [
    "# Inhalt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42e653b",
   "metadata": {},
   "source": [
    "<a href=#bookmark1> Folie 6: Recap Lineare Regression</a><br></br>\n",
    "<a href=#bookmark2> Folie 8: Lineare Regression auf nur 2 Datenpunkten</a>\n",
    "<BLOCKQUOTE><a href=#bookmark5> Zusatz zur Verdeutlichung der Modellvarianz</a></BLOCKQUOTE>\n",
    "<a href=#bookmark3> Folie 16: Ridge Regression</a>\n",
    "<BLOCKQUOTE><a href=#bookmark6> Fortsetzung Zusatz zur Verdeutlichung der Varianzreduktion durch Ridge Regression</a></BLOCKQUOTE>\n",
    "<a href=#bookmark4> Folie 17: Ridge Regression, Lasso Regression & Elastic Net</a><br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf8b59c",
   "metadata": {},
   "source": [
    "## Folie 6: Recap Lineare Regression <a name='bookmark1' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8aa85",
   "metadata": {},
   "source": [
    "Untersuchung des linearen Zusammenhangs einer abhängigen Variable $y$ und einer (**univariat**) oder mehrerer (**multivariat**) unabhängiger Variablen $x_j$.     \n",
    "<div class=\"fancy-box formel\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    $$ y(x) = w_0 + w_1 x_1 + ... + w_p x_p = w_0 + \\sum_{j=1}^p w_j x_j $$.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5e01b",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box python\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    Im folgenden Code wird beispielhaft der Zusammenhang zwischen Berufserfahrung in Jahren und Gehalt in tEUR geschätzt. Die Daten sind künstlich  erzeugt. <br/><br/>\n",
    "    Die Parameter der Geradengleichung $y(x) = w_0 + w_1 x_1 $ werden nach der Methode der kleinsten Quadrate so gewählt, dass die Summe der quadratischen Abweichungen der Datenpunkte zur Geraden minimal wird. \n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier findet das Modelltraining statt\n",
    "# Die Parameter w_0 und w_1 für die Regressionsgerade werden gemäß der Methode der kleinsten Quadrate so gewählt, \n",
    "# dass die Summe der quadratischen Abweichungen der Datenpunkte zur Geraden minimal wird\n",
    "linreg = LinearRegression().fit(X, y)\n",
    "\n",
    "# Visualisierung der Regressionsgeraden und der Datenpunkte\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(-0.5, 20.5)\n",
    "ax.set_ylim(16, 170)\n",
    "\n",
    "plt.scatter(X, y, marker= 'o', s=40, alpha=0.8)\n",
    "plt.plot(X, linreg.coef_ * X + linreg.intercept_, 'r-')\n",
    "\n",
    "plt.title('Lineare Regression')\n",
    "plt.xlabel('Berufserfahrung in Jahren (x)')\n",
    "plt.ylabel('Gehalt in tEUR (y)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "t = PrettyTable(['Geradengleichung', 'MqA'])\n",
    "mqa = round(np.mean((linreg.predict(X) - y) ** 2))\n",
    "equ = 'y(x) = '+str(round(linreg.intercept_,1))+' + '+str(round(linreg.coef_[0],1))+'x'\n",
    "t.add_row([equ, mqa])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1a54a",
   "metadata": {},
   "source": [
    "## Folie 8: Lineare Regression auf nur 2 Datenpunkten <a name='bookmark2' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabfe8a",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box question\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "Was wäre, wenn nur 2 Datenpunkte aus unserem Datensatz zur Schätzung zur Verfügung stehen würden (d.h. $n \\approx p$)? \n",
    "\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e8a87",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box python\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    Im folgenden Code werden 2 Beispiele erzeugt, indem aus dem zuvor generierten Datensatz für jedes Beispiel 2 Datenpunkte zufällig gezogen und eine lineare Regression durchgeführt wird. Dies simuliert den Fall, dass nur annähernd so viele Beobachtungen wie Attribute (hier: Berufserfahrung) zur Verfügung stehen. <br/><br/>\n",
    "    Auch wenn das nicht dem hochdimensionalen Fall entspricht, so veranschaulicht es doch die Probleme, die mit hochdimensionalen Daten auftreten.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_color = ['red', 'blue']\n",
    "\n",
    "t = PrettyTable(['Farbe','Geradengleichung', 'MqA (Training)', 'MqA (Test)']) #Definiere Table zur Ausgabe der Ergebnisse\n",
    "fig, ax = plt.subplots(1,2,figsize=(20, 5)) # Generierung einer Bildinstanz\n",
    "\n",
    "#Ein Foor Loop für jedes der beiden Beispiele\n",
    "for i in range(len(fig.get_axes())):\n",
    "    X_example = np.concatenate((np.array([X[point[i][0]]]), np.array([X[point[i][1]]])),axis = 0) # 2 Datenpunkte x-Werte\n",
    "    y_example = np.concatenate((np.array([y[point[i][0]]]), np.array([y[point[i][1]]])),axis = 0) # 2 Datenpunkte y-Werte\n",
    "    \n",
    "    # Durchführung der linearen Regression (Modelltraining)\n",
    "    linreg = LinearRegression().fit(X_example, y_example) \n",
    "    \n",
    "    #Visualisierung der Ergebnisse\n",
    "    ax[i].scatter(X, y, marker= 'o', s=20, alpha=0.2)\n",
    "    ax[i].scatter(X_example, y_example, marker= 'o', s=40, alpha=0.8)\n",
    "    ax[i].plot(X, linreg.coef_ * X + linreg.intercept_, c = line_color[i])\n",
    "    ax[i].set_xlabel('Berufserfahrung in Jahren (x)')\n",
    "    ax[i].set_ylabel('Gehalt in tEUR (y)')\n",
    "    ax[i].set_xlim(-0.5, 20.5)\n",
    "    ax[i].set_ylim(16, 170)\n",
    "    \n",
    "    #Berechnung der Mittleren quadratischen Abweichungen für Trainingsdaten und Testdaten\n",
    "    mqa_train = round(np.mean((linreg.predict(X_example) - y_example) ** 2))\n",
    "    mqa_test = round(np.mean((linreg.predict(np.delete(X, point[i], axis=0)) - np.delete(y, point[i], axis=0)) ** 2))\n",
    "    \n",
    "    #Bestimmung der Geradengleichung\n",
    "    if round(linreg.coef_[0]) > 0:\n",
    "        equ = str(round(linreg.intercept_,1))+' + '+str(abs(round(linreg.coef_[0],1)))+' x'     \n",
    "    else:\n",
    "        equ = str(round(linreg.intercept_,1))+' - '+str(abs(round(linreg.coef_[0],1)))+' x' \n",
    "        \n",
    "    t.add_row([line_color[i], equ, mqa_train, mqa_test])\n",
    "\n",
    "fig.suptitle('Lineare regression mit nur 2 Datenpunkten (2 Beispiele)',fontsize = 20)\n",
    "\n",
    "plt.show()\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83d83e4",
   "metadata": {},
   "source": [
    "### Zusatz zur Verdeutlichung der Modellvarianz (nicht in Foliensatz) <a name='bookmark5' />\n",
    "<div class=\"fancy-box python\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    Der folgende Code visualisiert noch einmal die hohe Varianz und das Overfitting, wenn das lineare Regressionsmodell nur\n",
    "    auf 2 Datenpunkten geschätzt wird.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Festlegung, wie viele Regressionen auf zwei zufällig gezogenen Datenpunkten aus unserem Datensatze durchgeführt werden\n",
    "number_examples = 5\n",
    "\n",
    "# Instanziierung der linearen Regression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "# Definition einer Funktion zur Durchführung der Regression, damit diese später im Notebook wieder verwendet werden kann\n",
    "def regression_analysis(number_examples, model, title):\n",
    "    \n",
    "    #Definiere Tabelle zur Ausgabe der Ergebnisse\n",
    "    t = PrettyTable(['Nummer', 'Geradengleichung', 'MqA (Training)', 'MqA (Test)']) \n",
    "    \n",
    "    color = color = [ cm.jet(x) for x in np.linspace(0, 1, number_examples) ]# Definition der Linienfarbe im Graph\n",
    "    #Definition des Graphen\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlim(-0.5, 20.5)\n",
    "    ax.set_ylim(16, 170)\n",
    "    #In jedem Schleifendurchlauf wird eine Regression auf 2 zufälligen Punkten aus dem Datensatz durchgeführt\n",
    "    #und visualisiert\n",
    "    for i in range(number_examples):\n",
    "\n",
    "        # Aufteilung des Datensets in Trainings- und Testdaten, sodass jeweils 2 Datenpunkte zum Training verwendet werden\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = i+15, test_size = 0.93)\n",
    "\n",
    "        # Durchführung der linearen Regression (Modelltraining)\n",
    "        model = model.fit(X_train, y_train) \n",
    "\n",
    "        # Visualisierung der Regressionsgerade\n",
    "        plt.plot(X, model.coef_ * X + model.intercept_, c = color[i], label=\"{}\".format(i+1))\n",
    "\n",
    "        # Berechnung des Trainings- und Testfehlers\n",
    "        mqa_train = round(np.mean((model.predict(X_train) - y_train) ** 2))\n",
    "        mqa_test = round(np.mean((model.predict(X_test) - y_test) ** 2))\n",
    "\n",
    "        # Aufstellen der Gleichung für die Regressionsgerade\n",
    "        if round(model.coef_[0]) > 0:\n",
    "            equ = str(round(model.intercept_,1))+' + '+str(abs(round(model.coef_[0],1)))+' x'     \n",
    "        else:\n",
    "            equ = str(round(model.intercept_,1))+' - '+str(abs(round(model.coef_[0],1)))+' x' \n",
    "\n",
    "        # Schreiben der Ergebnisse in die Tabelle zur Ausgabe\n",
    "        t.add_row([str(i+1), equ, mqa_train, mqa_test])\n",
    "\n",
    "\n",
    "    # Visualisierung der Datenpunkte, Beschriftung der Achsen und Ausgabe der Ergebnisse\n",
    "    plt.scatter(X, y, marker= 'o', s=40, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Berufserfahrung in Jahren (x)')\n",
    "    plt.ylabel('Gehalt in tEUR (y)')    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(t)\n",
    "    \n",
    "regression_analysis(number_examples = number_examples, model = linreg, title = 'Lineare Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7585ca3e",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box definition\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "Der Trainingsfehler der 5 linearen Regressionsschätzungen ist aufgrund des Overfittings immer 0 (MqA (Training)). Der Testfehler ist allerdings sehr hoch. Auch die hohe Varianz wird deutlich, da eine Änderung des Trainingsdatensatzes großen Einfluss auf das geschätzte Modell hat.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031a31e",
   "metadata": {},
   "source": [
    "## Folie 16: Ridge Regression <a name='bookmark3' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1237ebd7",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box formel\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "Kostenfunktion mit Penalty-Term bei der Ridge Regression\n",
    "    \\begin{equation} \\sum_{i=1}^n(y_i-w_0-\\sum_{j=1}^p w_{j}x_{ij})^2+\\lambda\\sum_{j=1}^p w_j^2\\end{equation}\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c8971",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lin_Reg.png\" style=\"width: 400px; padding: 30px\" img align=\"right\">\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<div class=\"fancy-box python\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    Im folgenden Code wird eine Ridge Regression auf den 2 Datenpunkten aus unserem oberen Beispiel (s. Graph rechts) \n",
    "    in Abhängigkeit von dem Tuning Parameter $\\lambda$ geschätzt.<br/><br/>\n",
    "    Spielen Sie gerne mit den Werten für $\\lambda$ (Zeile 12), um die Auswirkungen auf das Ergebnis zu sehen!\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb954eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiere Tabelle zur Ausgabe der Ergebnisse\n",
    "t = PrettyTable(['lambda', 'Geradengleichung', 'MqA (Training)', 'MqA (Test)', 'Penalty'])\n",
    "\n",
    "#Das sind die 2 Datenpunkte aus unserem oberen Beispiel\n",
    "X_example = np.concatenate((np.array([X[point[0][0]]]), np.array([X[point[0][1]]])),axis = 0) # 2 Datenpunkte x-Werte\n",
    "y_example = np.concatenate((np.array([y[point[0][0]]]), np.array([y[point[0][1]]])),axis = 0) # 2 Datenpunkte y-Werte\n",
    "\n",
    "# Verändern Sie die Werte und schauen Sie sich die Auswirkungen an\n",
    "lambdas = [0,1,10,100, 1000] \n",
    "\n",
    "\n",
    "color = [ cm.jet(x) for x in np.linspace(0, 1, len(lambdas)) ] # Definition der Linienfarbe im Graph\n",
    "\n",
    "#Definition des Graphen\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlim(-0.5, 20.5)\n",
    "ax.set_ylim(16, 170)\n",
    "\n",
    "#In jedem Schleifendurchlauf wird die Ridge Regression auf den 2 Datenpunkten aus unserem Beispiel mit verschiedenen\n",
    "#Werten für lambda durchgeführt und visualisiert\n",
    "\n",
    "for i in range(len(lambdas)):\n",
    "    \n",
    "    # Durchführung der Ridge Regression (Modelltraining) auf den 2 Datenpunkten mit Wert lambda\n",
    "    linridge = Ridge(alpha=lambdas[i]).fit(X_example, y_example)   \n",
    "    \n",
    "    # Visualisierung der Regressionsgerade\n",
    "    plt.plot(X, linridge.coef_ * X + linridge.intercept_, c = color[i], label=\"lambda={}\".format(lambdas[i]))\n",
    "    \n",
    "    # Berechnung des Trainings- und Testfehlers\n",
    "    mqa_train = round(np.mean((linridge.predict(X_example) - y_example) ** 2))\n",
    "    mqa_test = round(np.mean((linridge.predict(np.delete(X, [rand1, rand2], axis=0)) - np.delete(y, [rand1, rand2], axis=0)) ** 2))\n",
    "    \n",
    "    # Berechnung des Penalty Terms\n",
    "    penalty = sum(np.round(linridge.coef_,1)*lambdas[i])\n",
    "    \n",
    "    # Aufstellen der Gleichung für die Regressionsgerade\n",
    "    if round(linridge.coef_[0]) > 0:\n",
    "        equ = str(round(linridge.intercept_,1))+' + '+str(abs(round(linridge.coef_[0],1)))+' x'     \n",
    "    else:\n",
    "        equ = str(round(linridge.intercept_,1))+' - '+str(abs(round(linridge.coef_[0],1)))+' x' \n",
    "    \n",
    "    # Schreiben der Ergebnisse in die Tabelle zur Ausgabe\n",
    "    t.add_row([lambdas[i], equ, mqa_train, mqa_test, penalty])\n",
    "    \n",
    "# Visualisierung der Datenpunkte, Beschriftung der Achsen und Ausgabe der Ergebnisse   \n",
    "plt.scatter(X, y, marker= 'o', s=20, alpha=0.2)\n",
    "plt.scatter(X_example, y_example, marker= 'o', s=40, alpha=0.8)\n",
    "plt.title('Ridge Regression')\n",
    "plt.xlabel('Berufserfahrung in Jahren (x)')\n",
    "plt.ylabel('Gehalt in tEUR (y)')\n",
    "plt.legend()\n",
    "print(t)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe37724f",
   "metadata": {},
   "source": [
    "### Fortsetzung Zusatz zur Verdeutlichung der Varianzreduktion durch Ridge Regression (nicht in Foliensatz) <a name='bookmark6' />\n",
    "<img src=\"assets/Ridge.png\" style=\"width: 500px; padding: 30px\" img align=\"right\">\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<div class=\"fancy-box python\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    Aus unsere 5 zufällig generierten Regressionsprobleme mit jeweils 2 Datenpunkte aus unserem Datensatz wenden wir im folgenden die Ridge Regression an.<br/><br/>\n",
    "    Achten Sie dabei darauf, was mit den Geraden passiert und wie sich der Trainings- und Testfehler ändert.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiere Tabelle zur Ausgabe der Ergebnisse\n",
    "t = PrettyTable(['Nummer', 'Geradengleichung', 'MqA (Training)', 'MqA (Test)']) \n",
    "# Instanziierung der Ridge Regression. \n",
    "# Ändern Sie den Parameter alpha in der Funktion (entspricht lambda im Penalty-Term), um desse Einfluss zu sehen.\n",
    "linridge = Ridge(alpha = 10)\n",
    "\n",
    "# Ausführung der Funktion \"regression_analysis\" mit der Ridge Regression\n",
    "regression_analysis(number_examples = number_examples, model = linridge, title = 'Ridge Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4407cc",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box definition\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "Beispiel für $\\lambda$ = 10: <p/>\n",
    "    Besonders die Geraden 1, 3 und 5 passen deutlich besser zu unserm Datensatz als mit der normalen linearen Regression. Der Trainngsfehler ist größer geworden wohingegen der Testfehler abgenommen hat. Der Parameter $w_1$ ist durch den Penalty Term \"geschrumpft\" und die Varianz hat abgenommen.<br/><br/>\n",
    "    <b>Aber Achtung</b>, nicht in allen Fällen liefert die Ridge Regression ein besseres Ergebnis (s. Gerade 2)\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20716863",
   "metadata": {},
   "source": [
    "## Folie 17: Ridge Regression, Lasso Regression & Elastic Net <a name='bookmark4' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3b777",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"fancy-box formel\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "Penalty-Term für die Ridge Regression (L<sub>2</sub>-Norm)\n",
    "    \\begin{equation} \\lambda\\sum_{j=1}^p w_j^2\\end{equation}\n",
    "Penalty-Term für die Lasso Regression (L<sub>1</sub>-Norm)\n",
    "    \\begin{equation} \\lambda\\sum_{j=1}^p |w_j|\\end{equation}\n",
    "Penalty-Term für das Elastic Net\n",
    "    \\begin{equation} \\lambda(1-\\alpha)\\sum_{j=1}^p |w_j|+\\lambda\\alpha\\sum_{j=1}^p w_j^2\\end{equation}\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84fd1e8",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box python\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "    Im folgenden Code wird der Parameter $w$ als Funktion des Tuning Parameters $\\lambda$ für die Ridge Regression, Lasso Regression und das Elastic Net dargestellt.<br/><br/>\n",
    "    Im unteren Graph ist die Skalierung der Abszisse geändert, sodass Sie den Bereich, in dem die Lasso Regression und das Elastic Net einen \"Knick\" verursachen, genauer betrachten können.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53440811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition des Graphen\n",
    "fig, ax = plt.subplots(2,1,figsize=(5, 10))\n",
    "fig.tight_layout(h_pad=6)\n",
    "\n",
    "#Anzahl an verschiedenen lambda Parameter\n",
    "n_alphas = 200\n",
    "\n",
    "#Definition der lambda Parameter\n",
    "alphas = np.array([np.logspace(-4, 4, n_alphas), np.logspace(1, 3, n_alphas)])\n",
    "\n",
    "#Iteration durch alle lambda parameter und Ausführung der verschiedenen Regression mit diesen Parametern\n",
    "for i in range(len(alphas)):\n",
    "    \n",
    "    coefs_r = []\n",
    "    coefs_l = []\n",
    "    coefs_e = []\n",
    "    for a in alphas[i]:\n",
    "        #Ridge Regression\n",
    "        ridge = Ridge(alpha=a)\n",
    "        ridge.fit(X_example, y_example)\n",
    "        #Speicherung des Parameter w für bestimmtes lambda\n",
    "        coefs_r.append(ridge.coef_)\n",
    "        \n",
    "        #Lasso Regression\n",
    "        lasso = Lasso(alpha=a)\n",
    "        lasso.fit(X_example, y_example)\n",
    "        #Speicherung des Parameter w für bestimmtes lambda\n",
    "        coefs_l.append(lasso.coef_)\n",
    "        \n",
    "        #Elastic Net Regression\n",
    "        elasticNet = ElasticNet(alpha=a, l1_ratio=0.5)\n",
    "        elasticNet.fit(X_example, y_example)\n",
    "        #Speicherung des Parameter w für bestimmtes lambda\n",
    "        coefs_e.append(elasticNet.coef_)\n",
    "    \n",
    "    #Visualisierung\n",
    "    coefs = np.concatenate(([coefs_r, coefs_l, coefs_e]),axis = 1)\n",
    "    ax[i].plot(alphas[i], coefs)\n",
    "    ax[i].set_xscale(\"log\")\n",
    "    ax[i].legend(['Ridge', 'Lasso', 'Elastic Net'])\n",
    "    ax[i].set_xlabel('lambda')\n",
    "    ax[i].set_ylabel('Parameter w')\n",
    "    ax[i].set_ylim(-0.5, 25)\n",
    "    \n",
    "ax[1].set_title('Geaenderte Skalierung der Abszisse, \\nsodass der Bereich des \"Knicks\" gut sichtbar ist', fontsize = 10)\n",
    "fig.suptitle(\"Parameter w als Funktion von lambda\",fontsize = 10) \n",
    "plt.subplots_adjust(top=0.95)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dbc758",
   "metadata": {},
   "source": [
    "<div class=\"fancy-box definition\">\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "<div class=\"fancy-box__text\">\n",
    "Die Lasso Regression und das Elastic Net (Kombination von Lasso Regression und Ridge Regression) können zur Feature Selection genutzt werden, da hierbei Parameter null werden können.\n",
    "</div>\n",
    "<div class=\"fancy-box__img\"></div>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
